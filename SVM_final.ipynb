{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2490,
     "status": "ok",
     "timestamp": 1673473026658,
     "user": {
      "displayName": "Gustavo Nicoletti Rosa",
      "userId": "08068668883689258078"
     },
     "user_tz": -60
    },
    "id": "ymy6PU9CEx9R",
    "outputId": "ea55ae26-f8ca-4eb6-94e4-2e6e1640d457"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import resample\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from librosa import yin\n",
    "from librosa.effects import time_stretch\n",
    "from librosa.effects import trim\n",
    "from librosa.effects import pitch_shift\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1673473129124,
     "user": {
      "displayName": "Gustavo Nicoletti Rosa",
      "userId": "08068668883689258078"
     },
     "user_tz": -60
    },
    "id": "uilAP_gBPc4F",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## functions def\n",
    "\n",
    "nominal = [\"gender\", \"ageRange\", \"First Language spoken\", \"Current language used for work/school\"]\n",
    "# nominal = [\"gender\", \"ageRange\"]\n",
    "nominal = [\"ageRange\"]\n",
    "\n",
    "ordinal = [\"Self-reported fluency level \"]\n",
    "# global variables for encoding and decoding\n",
    "\n",
    "\n",
    "def create_encoders(df_original):\n",
    "    \"\"\"Creates the Encoders for\n",
    "    OneHotEncoding for nominal categorial data and\n",
    "    OrdinalEncoding for ordinal categorical data\n",
    "    and LabelEncoding for the Class label\"\"\"\n",
    "\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    ohe.fit(df_original[nominal])\n",
    "    fluency_enc = OrdinalEncoder(categories=[[\"basic\", \"intermediate\", \"advanced\", \"native\"]])\n",
    "    fluency_enc.fit([[\"basic\"], [\"intermediate\"], [\"advanced\"], [\"native\"]])\n",
    "    df_original[\"label\"]=df_original[\"action\"]+df_original[\"object\"]\n",
    "    class_enc = LabelEncoder()\n",
    "    class_enc.fit(df_original[\"label\"])\n",
    "\n",
    "    return df_original, ohe, fluency_enc, class_enc\n",
    "\n",
    "\n",
    "def encode_x(df, ohe, fluency_enc):\n",
    "    \"\"\"Encodes X\"\"\"\n",
    "    matrix = ohe.transform(df[nominal])\n",
    "    np.hstack((matrix, fluency_enc.transform(df[ordinal])))    \n",
    "    return matrix\n",
    "\n",
    "\n",
    "def feature_extraction_1(df, db=20):\n",
    "    \"\"\"Loads the audio data and extracts number of samples, max aplitude, and pitch\"\"\"\n",
    "    Xtime = []\n",
    "    samples_per_audio = []\n",
    "    max_amp_per_audio = []\n",
    "    pitch_per_audio = []\n",
    "    for i, files in tqdm(enumerate(df[\"path\"].values)):\n",
    "        audio = wavfile.read(files)\n",
    "        if audio[0]==22050:\n",
    "            audio = np.array(resample(audio[1], int(len(audio[1]) * (16000 / 22050))), dtype=\"int16\")\n",
    "        else:\n",
    "            audio = audio[1]\n",
    "        max_amp_per_audio.append(np.max(audio))\n",
    "        audio = audio.astype(\"float32\")\n",
    "        audio = audio[:64000]\n",
    "        audio = trim(audio, top_db=db)[0]\n",
    "        samples_per_audio.append(len(audio))\n",
    "        audio = time_stretch(audio, rate=len(audio)/21000)\n",
    "        pitch_per_audio.append(yin(audio, 50, 300,sr=16000))\n",
    "        Xtime.append(audio)\n",
    "    return np.array(Xtime), np.array(samples_per_audio), np.array(max_amp_per_audio), np.array(pitch_per_audio)\n",
    "\n",
    "\n",
    "def feature_extraction_2(Xtime, param={\"sr\":16000, \"n_fft\":2048, \"hop_length\":512, \"fmin\":50, \"n_mfcc\":10}):\n",
    "    \"\"\"Extracts the MFCC and Deltas from the Mel Spectogram\"\"\"\n",
    "    mfccs = []\n",
    "    deltas = []\n",
    "    deltas2 = []\n",
    "    for audio in Xtime:\n",
    "        mfcc = librosa.feature.mfcc(audio, **param)\n",
    "        delta = librosa.feature.delta(mfcc)\n",
    "        delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "        mfccs.append(mfcc.flatten())\n",
    "        deltas.append(delta.flatten())\n",
    "        deltas2.append(delta2.flatten())\n",
    "    coeff = np.hstack((mfccs, deltas, deltas2))\n",
    "    return coeff\n",
    "\n",
    "\n",
    "def data_augmentation(df, Xtime, Xenc, sam, max, y):\n",
    "    Xtime2 = []\n",
    "    Xtime3 = []\n",
    "    Xtime4 = []\n",
    "    for i, audio in enumerate(Xtime):\n",
    "        if df.loc[i, \"gender\"] == \"female\":\n",
    "            audio2 = pitch_shift(audio.astype(\"float32\"), sr = 16000, n_steps=-2)\n",
    "            audio3 = pitch_shift(audio.astype(\"float32\"), sr = 16000, n_steps=-4)\n",
    "            audio4 = pitch_shift(audio.astype(\"float32\"), sr = 16000, n_steps=-6)\n",
    "        else:\n",
    "            audio2 = pitch_shift(audio.astype(\"float32\"), sr = 16000, n_steps=2)\n",
    "            audio3 = pitch_shift(audio.astype(\"float32\"), sr = 16000, n_steps=4)\n",
    "            audio4 = pitch_shift(audio.astype(\"float32\"), sr = 16000, n_steps=6)\n",
    "        Xtime2.append(audio2)\n",
    "        Xtime3.append(audio3)\n",
    "        Xtime4.append(audio4)\n",
    "    Xtime = np.vstack((Xtime, Xtime2, Xtime3, Xtime4))\n",
    "    pit = []\n",
    "    for audio in Xtime:\n",
    "        pit.append(yin(audio, 50, 300,sr=16000))\n",
    "    pit = np.array(pit)\n",
    "    Xenc = np.vstack((Xenc, Xenc, Xenc, Xenc))\n",
    "    sam = np.hstack((sam, sam, sam, sam))\n",
    "    max = np.hstack((max, max, max, max))\n",
    "    y = np.hstack((y, y, y, y))\n",
    "    return Xtime, Xenc, sam, max, pit, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot of MFCC.png\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "\"\"\"\n",
    "par = {\"sr\":16000, \"n_fft\":2048, \"hop_length\":512, \"fmin\":50, \"n_mfcc\":10}\n",
    "config = {'C': 2, 'gamma': 'auto'}\n",
    "# df = pd.read_csv(\"/content/drive/MyDrive/project/dsl_data/development.csv\")\n",
    "df = pd.read_csv(\"development.csv\")\n",
    "Xtime, samples_per_audio, max_amp_per_audio, pitch_per_audio = feature_extraction_1(df)\n",
    "\n",
    "mfcc = librosa.feature.mfcc(Xtime[506], **par)\n",
    "mfcc_delta = librosa.feature.delta(mfcc)\n",
    "mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "fig, ax = plt.subplots(nrows=3, sharex=True, sharey=False)\n",
    "img1 = librosa.display.specshow(mfcc, ax=ax[0], x_axis='ms', y_axis='mel')\n",
    "ax[0].set(title='MFCC')\n",
    "ax[0].label_outer()\n",
    "img2 = librosa.display.specshow(mfcc_delta, ax=ax[1], x_axis='ms', y_axis='mel')\n",
    "ax[1].set(title=r'MFCC-$\\Delta$')\n",
    "ax[1].label_outer()\n",
    "img3 = librosa.display.specshow(mfcc_delta2, ax=ax[2], x_axis='ms', y_axis='mel')\n",
    "ax[2].set(title=r'MFCC-$\\Delta$-$\\Delta$')\n",
    "ax[2].label_outer()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.colorbar(img1, ax=[ax[0]])\n",
    "fig.colorbar(img2, ax=[ax[1]])\n",
    "fig.colorbar(img3, ax=[ax[2]])\n",
    "plt.savefig(f\"mfcc.png\")\n",
    "\n",
    "plt.show()\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "\"\"\"makeitcooler = wavfile.read(df[\"path\"].values[506])[1]\n",
    "# makeitcooler = makeitcooler/np.max(makeitcooler)\n",
    "makefft = fft(makeitcooler)[:8000]\n",
    "amp_fft = np.abs(makefft**2)\n",
    "db_fft = 20*np.log10(amp_fft)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(db_fft, linewidth=0.2)\n",
    "plt.xlim((-50,8050))\n",
    "plt.grid(alpha=0.2)\n",
    "plt.title(\"Energy spectrum density\")\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Magnitude (db)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"fft.png\")\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Xtime = []\n",
    "samples_per_audio = []\n",
    "for i, files in tqdm(enumerate(df[\"path\"].values)):\n",
    "    audio = wavfile.read(files)\n",
    "    if audio[0]==22050:\n",
    "        audio = np.array(resample(audio[1], int(len(audio[1]) * (16000 / 22050))), dtype=\"int16\")\n",
    "    else:\n",
    "        audio = audio[1]\n",
    "    audio = audio.astype(\"float32\")\n",
    "    # audio = trim(audio, top_db=db)[0]\n",
    "    samples_per_audio.append(len(audio))\n",
    "    # audio = time_stretch(audio, rate=len(audio)/21000)\n",
    "    Xtime.append(audio)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"plt.hist([len(audio)/16000 for audio in Xtime], bins=100)\n",
    "plt.grid(alpha=0.2)\n",
    "avg = sum([len(audio)/16000 for audio in Xtime])/len(Xtime)\n",
    "plt.axvline(x = avg, color='orange', linestyle=\"--\", label=\"average duration\")\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of duration of recordings before trimming\")\n",
    "plt.savefig(\"distr.png\")\n",
    "plt.show()\n",
    "print(avg)\n",
    "\"\"\"\n",
    "\"\"\"Xtrim = []\n",
    "for audio in Xtime:\n",
    "    audio_trim = trim(audio, top_db=20)[0]\n",
    "    Xtrim.append(audio_trim)\n",
    "plt.hist([len(audio)/16000 for audio in Xtrim], bins=100)\n",
    "plt.grid(alpha=0.2)\n",
    "avg = sum([len(audio)/16000 for audio in Xtrim])/len(Xtrim)\n",
    "plt.axvline(x = avg, color='orange', linestyle=\"--\", label=\"average duration\")\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of duration of recordings after trimming\")\n",
    "plt.show()\n",
    "print(avg)\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2298,
     "status": "ok",
     "timestamp": 1673472219431,
     "user": {
      "displayName": "Gustavo Nicoletti Rosa",
      "userId": "08068668883689258078"
     },
     "user_tz": -60
    },
    "id": "pnznnV_ekHzX",
    "outputId": "5a7f6528-2ebf-4913-edc0-4b270ebae9f6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Preprocessing X train -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9854it [03:11, 51.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Training Model -----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight='balanced',\n",
       "                       max_features='log2', min_samples_split=10,\n",
       "                       n_estimators=1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## X test\n",
    "par = {'sr': 16000, 'n_fft': 2048, 'hop_length': 512, 'fmin': 50, 'n_mfcc': 10}\n",
    "config = {'C': 2}\n",
    "config = {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini',\n",
    "          'max_depth': None, 'max_features': 'log2', 'min_samples_split': 10, 'n_estimators': 1000}\n",
    "\n",
    "# df = pd.read_csv(\"/content/drive/MyDrive/project/dsl_data/development.csv\")\n",
    "df = pd.read_csv(\"development.csv\")\n",
    "# filter out categories not in x_test\n",
    "\"\"\"df = df[df[\"Self-reported fluency level \"]==\"native\"]\n",
    "df = df[df['ageRange']!=\"65+\"]\n",
    "df = df[df['Current language used for work/school']==\"English (United States)\"]\"\"\"\n",
    "\n",
    "# get fraction of database\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# create encoders\n",
    "df, ohe, fluency_enc, class_enc = create_encoders(df)\n",
    "print(\"----- Preprocessing X train -----\")\n",
    "# encode training set\n",
    "Xtrain_encoded = encode_x(df, ohe, fluency_enc)\n",
    "ytrain = class_enc.transform(df[[\"label\"]])\n",
    "Xtime, samples_per_audio, max_amp_per_audio, pitch_per_audio = feature_extraction_1(df)\n",
    "Xtime_train, Xencoded_train, samples_per_audio_train, max_amp_per_audio_train, pitch_per_audio_train, y_train = \\\n",
    "            data_augmentation(df, Xtime, Xtrain_encoded, samples_per_audio, max_amp_per_audio, ytrain)\n",
    "mfcc_delta_train = feature_extraction_2(Xtime_train, par)\n",
    "X_train = np.hstack((\n",
    "    mfcc_delta_train, Xencoded_train,\n",
    "    np.array(samples_per_audio_train)[:, np.newaxis],\n",
    "    np.array(max_amp_per_audio_train)[:, np.newaxis],\n",
    "    pitch_per_audio_train\n",
    "    ))\n",
    "std = StandardScaler()\n",
    "X_train = std.fit_transform(X_train)\n",
    "# train SVM\n",
    "print(\"----- Training Model -----\")\n",
    "svc = RandomForestClassifier(**config)\n",
    "svc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"durations = [len(audio) for audio in Xtime]\n",
    "plt.hist(durations,bins=100)\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Preprocessing X test -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1455it [00:26, 55.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Predicting X test -----\n"
     ]
    }
   ],
   "source": [
    "# X test\n",
    "print(\"----- Preprocessing X test -----\")\n",
    "df_test = pd.read_csv(\"evaluation.csv\")\n",
    "Xtest_encoded = encode_x(df_test, ohe, fluency_enc)\n",
    "Xtime_t, sam_t, max_t, pit_t = feature_extraction_1(df_test)\n",
    "mfcc_d_t = feature_extraction_2(Xtime_t, par)\n",
    "X_test = np.hstack((mfcc_d_t, Xtest_encoded, np.array(sam_t)[:, np.newaxis], np.array(max_t)[:, np.newaxis], pit_t))\n",
    "X_test = std.transform(X_test)\n",
    "print(\"----- Predicting X test -----\")\n",
    "y_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ ALL DONE ------\n"
     ]
    }
   ],
   "source": [
    "y_pred = class_enc.inverse_transform(y_pred)\n",
    "id = [int(i) for i in df_test[\"Id\"]]\n",
    "final = pd.DataFrame({\"Id\":id, \"Predicted\": y_pred})\n",
    "final = final.sort_values(by=[\"Id\"])\n",
    "final.to_csv(\"submission/g18.csv\", index=False)\n",
    "print(\"------ ALL DONE ------\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
