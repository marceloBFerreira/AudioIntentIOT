{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2490,
     "status": "ok",
     "timestamp": 1673473026658,
     "user": {
      "displayName": "Gustavo Nicoletti Rosa",
      "userId": "08068668883689258078"
     },
     "user_tz": -60
    },
    "id": "ymy6PU9CEx9R",
    "outputId": "ea55ae26-f8ca-4eb6-94e4-2e6e1640d457"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import resample\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from librosa import yin\n",
    "from librosa.effects import time_stretch\n",
    "from librosa.effects import trim\n",
    "from librosa.effects import pitch_shift\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1673473129124,
     "user": {
      "displayName": "Gustavo Nicoletti Rosa",
      "userId": "08068668883689258078"
     },
     "user_tz": -60
    },
    "id": "uilAP_gBPc4F"
   },
   "outputs": [],
   "source": [
    "## functions def\n",
    "\n",
    "# nominal = [\"gender\", \"ageRange\", \"First Language spoken\", \"Current language used for work/school\"]\n",
    "nominal = [\"gender\", \"ageRange\"]\n",
    "# nominal = [\"ageRange\"]\n",
    "ordinal = [\"Self-reported fluency level \"]\n",
    "# global variables for encoding and decoding\n",
    "\n",
    "\n",
    "def create_encoders(df_original):\n",
    "    \"\"\"Creates the Encoders for\n",
    "    OneHotEncoding for nominal categorial data and\n",
    "    OrdinalEncoding for ordinal categorical data\n",
    "    and LabelEncoding for the Class label\"\"\"\n",
    "\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    ohe.fit(df_original[nominal])\n",
    "    fluency_enc = OrdinalEncoder(categories=[[\"basic\", \"intermediate\", \"advanced\", \"native\"]])\n",
    "    fluency_enc.fit([[\"basic\"], [\"intermediate\"], [\"advanced\"], [\"native\"]])\n",
    "    df_original[\"label\"]=df_original[\"action\"]+df_original[\"object\"]\n",
    "    class_enc = LabelEncoder()\n",
    "    class_enc.fit(df_original[\"label\"])\n",
    "\n",
    "    return df_original, ohe, fluency_enc, class_enc\n",
    "\n",
    "\n",
    "def encode_x(df, ohe, fluency_enc):\n",
    "    \"\"\"Encodes X\"\"\"\n",
    "    matrix = ohe.transform(df[nominal])\n",
    "    # np.hstack((matrix, fluency_enc.transform(df[ordinal])))    \n",
    "    return matrix\n",
    "\n",
    "\n",
    "def feature_extraction_1(df, db=20):\n",
    "    \"\"\"Loads the audio data and extracts number of samples, max aplitude, and pitch\"\"\"\n",
    "    Xtime = []\n",
    "    samples_per_audio = []\n",
    "    max_amp_per_audio = []\n",
    "    pitch_per_audio = []\n",
    "    for i, files in tqdm(enumerate(df[\"path\"].values)):\n",
    "        audio = wavfile.read(files)\n",
    "        if audio[0]==22050:\n",
    "            audio = np.array(resample(audio[1], int(len(audio[1]) * (16000 / 22050))), dtype=\"int16\")\n",
    "        else:\n",
    "            audio = audio[1]\n",
    "        max_amp_per_audio.append(np.max(audio))\n",
    "        audio = audio.astype(\"float32\")\n",
    "        audio = audio[:64000]\n",
    "        audio = trim(audio, top_db=db)[0]\n",
    "        samples_per_audio.append(len(audio))\n",
    "        audio = time_stretch(audio, rate=len(audio)/21000)\n",
    "        pitch_per_audio.append(yin(audio, 50, 300,sr=16000))\n",
    "        Xtime.append(audio)\n",
    "    return np.array(Xtime), np.array(samples_per_audio), np.array(max_amp_per_audio), np.array(pitch_per_audio)\n",
    "\n",
    "\n",
    "def feature_extraction_2(Xtime, param={\"sr\":16000, \"n_fft\":2048, \"hop_length\":512, \"fmin\":50, \"n_mfcc\":10}):\n",
    "    \"\"\"Extracts the MFCC and Deltas from the Mel Spectogram\"\"\"\n",
    "    mfccs = []\n",
    "    deltas = []\n",
    "    deltas2 = []\n",
    "    for audio in Xtime:\n",
    "        mfcc = librosa.feature.mfcc(audio, **param)\n",
    "        delta = librosa.feature.delta(mfcc)\n",
    "        delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "        mfccs.append(mfcc.flatten())\n",
    "        deltas.append(delta.flatten())\n",
    "        deltas2.append(delta2.flatten())\n",
    "    coeff = np.hstack((mfccs, deltas, deltas2))\n",
    "    return coeff\n",
    "\n",
    "\n",
    "\"\"\"from skimage.measure import block_reduce\n",
    "def feature_extraction_2b(Xtime, param={\"sr\":16000, \"n_fft\":2048, \"hop_length\":512, \"fmin\":50, \"n_mfcc\":10},\n",
    "                          m=1, n=3, f = np.mean):\n",
    "    mfccs = []\n",
    "    deltas = []\n",
    "    deltas2 = []\n",
    "    for audio in Xtime:\n",
    "        mfcc = librosa.feature.mfcc(audio, **param)\n",
    "    \n",
    "        # pooling with matrix size MxN, where columns are times (n) \n",
    "        # and rows are frequencies (m)\n",
    "\n",
    "        delta = librosa.feature.delta(mfcc)\n",
    "        delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "        mfcc = block_reduce(mfcc, (m,n), f)\n",
    "        delta = block_reduce(delta, (m,n), f)\n",
    "        delta2 = block_reduce(delta2, (m,n), f)\n",
    "        mfccs.append(mfcc.flatten())\n",
    "        deltas.append(delta.flatten())\n",
    "        deltas2.append(delta2.flatten())\n",
    "        \n",
    "    coeff = np.hstack((mfccs, deltas, deltas2))\n",
    "    return coeff\n",
    "\"\"\"\n",
    "           \n",
    "def data_augmentation(df, indices, Xtime, Xenc, sam, max, y):\n",
    "    Xtime2 = []\n",
    "    Xtime3 = []\n",
    "    Xtime4 = []\n",
    "    for i, audio in enumerate(Xtime):\n",
    "        if df.loc[indices[i], \"gender\"] == \"female\":\n",
    "            audio2 = pitch_shift(audio.astype(\"float32\"), sr = 16000, n_steps=-2)\n",
    "            audio3 = pitch_shift(audio.astype(\"float32\"), sr = 16000, n_steps=-4)\n",
    "            audio4 = pitch_shift(audio.astype(\"float32\"), sr = 16000, n_steps=-6)\n",
    "        else:\n",
    "            audio2 = pitch_shift(audio.astype(\"float32\"), sr = 16000, n_steps=2)\n",
    "            audio3 = pitch_shift(audio.astype(\"float32\"), sr = 16000, n_steps=4)\n",
    "            audio4 = pitch_shift(audio.astype(\"float32\"), sr = 16000, n_steps=6)\n",
    "        Xtime2.append(audio2)\n",
    "        Xtime3.append(audio3)\n",
    "        Xtime4.append(audio4)\n",
    "    Xtime = np.vstack((Xtime, Xtime2, Xtime3, Xtime4))\n",
    "    pit = []\n",
    "    for audio in Xtime:\n",
    "        pit.append(yin(audio, 50, 300,sr=16000))\n",
    "    pit = np.array(pit)\n",
    "    Xenc = np.vstack((Xenc, Xenc, Xenc, Xenc))\n",
    "    sam = np.hstack((sam, sam, sam, sam))\n",
    "    max = np.hstack((max, max, max, max))\n",
    "    y = np.hstack((y, y, y, y))\n",
    "    return Xtime, Xenc, sam, max, pit, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2298,
     "status": "ok",
     "timestamp": 1673472219431,
     "user": {
      "displayName": "Gustavo Nicoletti Rosa",
      "userId": "08068668883689258078"
     },
     "user_tz": -60
    },
    "id": "pnznnV_ekHzX",
    "outputId": "5a7f6528-2ebf-4913-edc0-4b270ebae9f6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## PREPARE CROSS VALIDATION DATASET\n",
    "# data reading\n",
    "# df = pd.read_csv(\"/content/drive/MyDrive/project/dsl_data/development.csv\")\n",
    "df = pd.read_csv(\"development.csv\")\n",
    "# filter out categories not in x_test\n",
    "\"\"\"df = df[df[\"Self-reported fluency level \"]==\"native\"]\n",
    "df = df[df['ageRange']!=\"65+\"]\n",
    "df = df[df['Current language used for work/school']==\"English (United States)\"]\"\"\"\n",
    "\n",
    "# get fraction of database \n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# create encoders\n",
    "df, ohe, fluency_enc, class_enc = create_encoders(df)\n",
    "\n",
    "# encode training set\n",
    "Xtrain_encoded = encode_x(df, ohe, fluency_enc)\n",
    "ytrain = class_enc.transform(df[[\"label\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRXh8GGU0JcP",
    "outputId": "9e277002-4c32-4010-83a7-22106591d90c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9854it [03:33, 46.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 out of 48\n",
      "{'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': None, 'max_features': 'log2', 'min_samples_split': 10, 'n_estimators': 1000}\n",
      "accuracy | balanced\n",
      "  0.7890 |   0.7650\n",
      "  0.7896 |   0.7633\n",
      "  0.8118 |   0.7939\n",
      "avg_acc = 0.7968; avg_bal = 0.7741\n",
      "best acc = 0.7968 - {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': None, 'max_features': 'log2', 'min_samples_split': 10, 'n_estimators': 1000}\n",
      "best bal = 0.7741 - {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': None, 'max_features': 'log2', 'min_samples_split': 10, 'n_estimators': 1000}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# CROSS VALIDATION\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from numpy.matrixlib.defmatrix import N\n",
    "\n",
    "best_acc_config = None\n",
    "best_bal_config = None\n",
    "best_acc = 0\n",
    "best_bal = 0\n",
    "i = 1\n",
    "I = 48\n",
    "Xtime, samples_per_audio, max_amp_per_audio, pitch_per_audio = feature_extraction_1(df)\n",
    "\n",
    "\"\"\"hyperparameters = {\n",
    "    \"n_estimators\":[1000],\n",
    "    \"max_depth\": [None],\n",
    "    \"criterion\":[\"gini\", \"entropy\"],\n",
    "    \"min_samples_split\":[2, 5, 10],\n",
    "    \"max_features\":[\"sqrt\", \"log2\"],\n",
    "    \"bootstrap\":[True, False],\n",
    "    \"class_weight\":[None, \"balanced\"]\n",
    "}\n",
    "\"\"\"\n",
    "configs = [{'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini',\n",
    "            'max_depth': None, 'max_features': 'log2', 'min_samples_split': 10, 'n_estimators': 1000}]\n",
    "params = [\n",
    "    {\"sr\":16000, \"n_fft\":2048, \"hop_length\":512, \"fmin\":50, \"n_mfcc\":10},\n",
    "]\n",
    "\n",
    "for par in params:\n",
    "    #for config in ParameterGrid(hyperparameters):\n",
    "    for config in configs:\n",
    "        print(f\"Iteration {i} out of {I}\")\n",
    "        # print(par)\n",
    "        print(config)\n",
    "        i += 1\n",
    "        kf = KFold(3, shuffle = True)\n",
    "        avg_acc = 0\n",
    "        avg_bal = 0\n",
    "        lens = []\n",
    "        print(\"accuracy | balanced\")\n",
    "        for train_indices, validation_indices in kf.split(Xtime, ytrain):\n",
    "            l = len(validation_indices)\n",
    "            lens.append(l)\n",
    "            # validation set\n",
    "            Xtime_valid = Xtime[validation_indices]\n",
    "            samples_per_audio_valid = samples_per_audio[validation_indices]\n",
    "            max_amp_per_audio_valid = max_amp_per_audio[validation_indices]\n",
    "            pitch_per_audio_valid = pitch_per_audio[validation_indices]\n",
    "            y_valid = ytrain[validation_indices]\n",
    "\n",
    "          # Data augmentation of X test\n",
    "            Xtime_train, Xencoded_train, samples_per_audio_train, max_amp_per_audio_train, pitch_per_audio_train, y_train = \\\n",
    "            data_augmentation(\n",
    "                df, train_indices, Xtime[train_indices], Xtrain_encoded[train_indices],\n",
    "                samples_per_audio[train_indices], max_amp_per_audio[train_indices], ytrain[train_indices]\n",
    "            )\n",
    "\n",
    "          # prep X_train\n",
    "            mfcc_delta_train = feature_extraction_2(Xtime_train, par)\n",
    "            X_train = np.hstack((\n",
    "                mfcc_delta_train, Xencoded_train,\n",
    "                np.array(samples_per_audio_train)[:, np.newaxis],\n",
    "                np.array(max_amp_per_audio_train)[:, np.newaxis],\n",
    "                pitch_per_audio_train\n",
    "                ))\n",
    "            std = StandardScaler()\n",
    "            X_train = std.fit_transform(X_train)\n",
    "\n",
    "          # prep X_valid\n",
    "            mfcc_delta_valid = feature_extraction_2(Xtime_valid, par)\n",
    "            X_valid = np.hstack((\n",
    "                mfcc_delta_valid, Xtrain_encoded[validation_indices],\n",
    "                np.array(samples_per_audio_valid)[:, np.newaxis],\n",
    "                np.array(max_amp_per_audio_valid)[:, np.newaxis],\n",
    "                pitch_per_audio_valid\n",
    "                ))\n",
    "            X_valid = std.transform(X_valid)\n",
    "\n",
    "            # train SVM\n",
    "            svc = RandomForestClassifier(**config)\n",
    "            svc.fit(X_train, y_train)\n",
    "            y_pred = svc.predict(X_valid)\n",
    "            acc = accuracy_score(y_valid, y_pred)\n",
    "            bal_acc = balanced_accuracy_score(y_valid, y_pred)\n",
    "            print(f\"{acc:8.4f} | {bal_acc:8.4f}\")\n",
    "            avg_acc += acc*l\n",
    "            avg_bal += bal_acc*l\n",
    "        avg_acc /= sum(lens)\n",
    "        avg_bal /= sum(lens)\n",
    "        print(f\"avg_acc = {avg_acc:.4f}; avg_bal = {avg_bal:.4f}\")\n",
    "        if avg_acc > best_acc:\n",
    "            best_acc = avg_acc\n",
    "            best_acc_config = config\n",
    "        if avg_bal > best_bal:\n",
    "            best_bal = avg_bal\n",
    "            best_bal_config = config\n",
    "print(f\"best acc = {best_acc:.4f} - {best_acc_config}\")\n",
    "print(f\"best bal = {best_bal:.4f} - {best_bal_config}\")\n",
    "print(\"DONE\")"
   ]
  }

 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
